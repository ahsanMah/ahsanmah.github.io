[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Attempt at blogging"
  },
  {
    "objectID": "posts/simplegnn-deux/index.html",
    "href": "posts/simplegnn-deux/index.html",
    "title": "Building Graph Neural Networks from Scratch - Part II",
    "section": "",
    "text": "In the previous post we built a GNN to reconstruct a single graph sample. In this part, we will consolidate our model into proper classes and build towards a graph classification network.\nWe load the graph dataset as before.\n# %pip install torch_geometric -q\nimport torch\nimport torch_geometric\nimport torch.nn as nn\nimport seaborn as sns\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\n\n\nDATASET_PATH = \"/tmp/data\"\n\n# Random number generator for later use\nrand_generator = torch.Generator()\nrand_generator.manual_seed(42)\ntorch.set_printoptions(precision=3)\n\ndataset = torch_geometric.datasets.TUDataset(\nroot=DATASET_PATH, name=\"MUTAG\",\nuse_edge_attr=True, use_node_attr=True\n)"
  },
  {
    "objectID": "posts/simplegnn-deux/index.html#sending-informative-messages",
    "href": "posts/simplegnn-deux/index.html#sending-informative-messages",
    "title": "Building Graph Neural Networks from Scratch - Part II",
    "section": "1 Sending informative messages",
    "text": "1 Sending informative messages\nWe will first create a message passing layer. Note that in the previous part we simply summed neighbouring features as a messaging scheme. This is far from expressive. In Message Passing Neural Nets (MPNNs), we ideally want to aggregate messages after a feature transformation: \\(m_v = \\sum_i f(w_i)\\). That is, the message \\(m_v\\) which will be sent to node \\(v\\) should be “collected” from all its neighboring nodes \\(w_i\\). We can also think of our previous messaging scheme as using the identity function: \\(f(w) = w\\).\n\nclass MessagePasser(torch.nn.Module):\n\n  def __init__(self, node_features, edge_features, use_identities=False):\n    super().__init__()\n    \n    if use_identities:\n      self.f_v = nn.Identity()\n      self.f_w = nn.Identity()\n      self.f_e = nn.Identity()\n    else:\n      self.f_v = nn.Linear(node_features, node_features)\n      self.f_w = nn.Linear(node_features, node_features)\n      self.f_e = nn.Linear(edge_features, node_features)\n\n  def __call__(self, v, w, e_vw):\n    '''\n    Pass information TO v FROM w, additionally using edge information\n    '''\n    h = self.f_v(v) + self.f_w(w) + self.f_e(e_vw)\n    \n    return h"
  },
  {
    "objectID": "posts/simplegnn-deux/index.html#implementing-a-souped-up-simplegnn",
    "href": "posts/simplegnn-deux/index.html#implementing-a-souped-up-simplegnn",
    "title": "Building Graph Neural Networks from Scratch - Part II",
    "section": "2 Implementing a souped up SimpleGNN",
    "text": "2 Implementing a souped up SimpleGNN\nLet’s use this new MessagePasser in our SimpleGNN model below. I have also included some additional modeling mechanisms to improve learning.\nFirst, note that we have a for loop in our forward function! This is my attempt at multiple message passing iterations. Ideally you would want each iteration to utilize additional layers, going ever deeper in the network. Alternatively you can reuse the existing layers for each iteration and call it “sharing” parameters across the network. At least that is my understanding.\nSecond, we introduce the concept of a “global” node. This will be a special node which will be connected to all other nodes (and edges) in the graph. As every node will get a message from the global node, it allows us to collect information from distal nodes in the graph in a relatively few message passing iterations.\n\nclass SimpleGNN(torch.nn.Module):\n\n    def __init__(self, input_node_features, input_edge_features, output_node_features, hidden_dims):\n        super().__init__()\n        torch.manual_seed(42)\n        self.node_featurizer = nn.Linear(input_node_features, hidden_dims)\n        self.edge_featurizer = nn.Linear(input_edge_features, hidden_dims)\n        self.messenger = MessagePasser(node_features=hidden_dims, edge_features=hidden_dims)\n        self.readout = nn.Linear(hidden_dims, output_node_features)\n    \n    def get_degrees(self, num_nodes, edge_list):\n        counts = torch.ones(num_nodes, 1)\n        idxs = edge_list[:,1].reshape(-1, 1)\n        ones = torch.ones(len(idxs), 1)\n        counts.scatter_add_(dim=0, index=idxs, src=ones)\n        return counts\n\n    def forward(self, nodes, edges, edge_list, message_passing_iters=1):\n        \n        neighbor_counts = self.get_degrees(len(nodes), edge_list)\n\n        # Get embeddings\n        node_embs = self.node_featurizer(nodes)\n        edge_embs = self.edge_featurizer(edges)\n\n        # Special nodes and edges\n        global_node = torch.ones(len(nodes),input_node_features, requires_grad=False)\n        global_edge = torch.ones(len(nodes), input_edge_features, requires_grad=False)\n\n        global_node_embedding = self.node_featurizer(global_node)\n        global_edge_embedding = self.edge_featurizer(global_edge)\n        \n        for _ in range(message_passing_iters):\n            # Update global / 'master' node\n            global_edge_embedding = self.messenger(node_embs, global_node_embedding, global_edge_embedding)\n            global_node_embedding = self.messenger(global_node_embedding, node_embs, global_edge_embedding)\n\n            # Get edge list and compute messages\n            node_v_idxs, node_w_idxs = edge_list[:,0], edge_list[:,1]\n            v_embs, w_embs = node_embs[node_v_idxs], node_embs[node_w_idxs]\n            messages = self.messenger(v_embs, w_embs, edge_embs)\n\n            # master to nodes\n            node_embs = node_embs + global_node_embedding\n\n            # Get target node indices from edge list\n            num_edges, num_message_features = messages.shape\n            message_target_nodes = torch.zeros(num_edges, num_message_features,\n                                                dtype=torch.long, requires_grad=False)\n            message_target_nodes[:] = node_v_idxs.reshape(-1,1)\n\n            # Apply message updates from w to v\n            # i.e. edges to node\n            node_embs = torch.scatter_reduce(node_embs,\n                dim=0, index=message_target_nodes,\n                src=messages, reduce='sum', include_self=True\n            )\n\n            # Normalize by number of neighbors\n            node_embs = node_embs / neighbor_counts\n\n        # Get graph outputs\n        graph_output = self.readout(node_embs)\n\n        return graph_output\n\nBelow, we train our model to reconstruct the input as in Part 1. Note that we are using the same number of hidden dimensions as before, but iterating for 2000 iterations instead of 10,000. The learning rate is also set to 1e-3 instead of 1e-2.\n\ndef mse_step_fn(nodes, edges, edge_list, gnn, opt, message_passing_iters=1):\n  opt.zero_grad()\n  \n  # Get reconstruction\n  node_reconstruction = gnn(nodes, edges, edge_list, message_passing_iters)\n\n  # Compute reconstruction loss\n  loss = (nodes - node_reconstruction).square().sum(axis=1)\n  loss = loss.mean()\n  loss.backward()\n  opt.step()\n\n  return loss.item()\n\nhidden_dims = 3\ninput_node_features = 7\ninput_edge_features = 4\n\nsgnn = SimpleGNN(input_node_features, input_edge_features, input_node_features, hidden_dims)\nopt = torch.optim.AdamW(sgnn.parameters(), lr=1e-3)\nsgnn\n\nSimpleGNN(\n  (node_featurizer): Linear(in_features=7, out_features=3, bias=True)\n  (edge_featurizer): Linear(in_features=4, out_features=3, bias=True)\n  (messenger): MessagePasser(\n    (f_v): Linear(in_features=3, out_features=3, bias=True)\n    (f_w): Linear(in_features=3, out_features=3, bias=True)\n    (f_e): Linear(in_features=3, out_features=3, bias=True)\n  )\n  (readout): Linear(in_features=3, out_features=7, bias=True)\n)\n\n\n\n\nWe train our updated model with a reconstruction loss as before.\nsample = dataset[0]\nnodes = sample.x\nedges = sample.edge_attr\nedge_list = sample.edge_index.T\n\nfor i in range(2_001):\n    \n    loss = mse_step_fn(nodes, edges, edge_list, sgnn, opt, message_passing_iters=1)\n    \n    if i % 500 == 0:\n        print(f\"Step {i:4d}: {loss:.3f}\")\n\nnode_reconstruction = sgnn(nodes, edges, edge_list).detach()\nfig, axs = plt.subplots(1,2, figsize=(12,4))\nsns.heatmap(nodes, ax=axs[0]);\naxs[0].set_title(\"Original\")\nsns.heatmap(node_reconstruction, ax=axs[1]);\naxs[1].set_title(\"Reconstruction\");\n\n\nStep    0: 2.717\nStep  500: 0.108\nStep 1000: 0.028\nStep 1500: 0.010\nStep 2000: 0.003\n\n\n\n\n\n\n\n\n\nNote the near perfect reconstruction at a fifth of the time steps as before! As we only perform a single round of message passing, this implies that we got phenomenal gains from using global embeddings and transformation functions alone! Note that our model is still exclusively linear transformations. We have yet to use ReLU or some other fancy activation function."
  },
  {
    "objectID": "posts/simplegnn-deux/index.html#building-a-classifier",
    "href": "posts/simplegnn-deux/index.html#building-a-classifier",
    "title": "Building Graph Neural Networks from Scratch - Part II",
    "section": "3 Building a Classifier",
    "text": "3 Building a Classifier\nWe can utilize our newly created architecture for building a classifier as well. As long as we have a mechanism of pooling the final embedding of the graph into logits (log-probabilities per class). Below, I sum up the embeddings of all nodes in the graph into produce a 2-dimensional output representing the logits. You may call this global average (sum?) pooling if you wish. I assume that the each node contains all the necessary information from the edges after the final layer in the network.\n\ndef bce_loss(logits, targets):\n    return F.binary_cross_entropy_with_logits(logits, F.one_hot(targets, num_classes=2).float())\n\ndef classification_step_fn(graph_sample, gnn, opt, message_passing_iters=1, train=True):\n  nodes, edges = graph_sample.x, graph_sample.edge_attr\n  edge_list = graph_sample.edge_index.T\n  gt_label =  graph_sample.y\n\n  # Get graph readout - per node\n  pred_per_node = gnn(nodes, edges, edge_list, message_passing_iters)\n\n  # Pool the embeddings from all nodes\n  graph_pred = pred_per_node.sum(dim=0, keepdim=True)\n\n  # Compute cross entropy loss\n  loss = bce_loss(graph_pred, gt_label)\n\n  if train:\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n\n  return loss.item()\n\n\n\nThe dataset can be split into train and test sets using an 80/20 split\nshuffle_idx = torch.randperm(n=len(dataset), generator=rand_generator)\ntest_num_samples = int(0.2 * len(dataset))\ntest_set = dataset[shuffle_idx[:test_num_samples]]\ntrain_set = dataset[shuffle_idx[test_num_samples:]]\n\n\nThe network will be trained in a similar manner as our regression model. I included a weight decay term to the optimizer to add a tiny bit of regularization.\n\nsgnn = SimpleGNN(input_node_features, input_edge_features, output_node_features=2, hidden_dims=32)\nopt = torch.optim.AdamW(sgnn.parameters(), lr=3e-4, weight_decay=1e-5)\n\nfor i in range(10_000):\n    idx = i % len(train_set)\n    graph_sample = train_set[idx]\n    loss = classification_step_fn(graph_sample, sgnn, opt, message_passing_iters=1)\n    \n    if i % 1000 == 0:\n        with torch.no_grad():\n            val_loss = 0.0\n            for x in test_set:\n              val_loss += classification_step_fn(x, sgnn, opt, train=False)\n            val_loss /= len(test_set)\n            print(f\"Iter: {i:&gt;4} - Train Loss: {loss:.3f} - Val Loss {val_loss:.3f}\")\n\nIter:    0 - Train Loss: 5.091 - Val Loss 3.436\nIter: 1000 - Train Loss: 1.704 - Val Loss 0.405\nIter: 2000 - Train Loss: 0.618 - Val Loss 0.380\nIter: 3000 - Train Loss: 0.396 - Val Loss 0.447\nIter: 4000 - Train Loss: 0.228 - Val Loss 0.401\nIter: 5000 - Train Loss: 0.358 - Val Loss 0.341\nIter: 6000 - Train Loss: 0.067 - Val Loss 0.419\nIter: 7000 - Train Loss: 0.512 - Val Loss 0.306\nIter: 8000 - Train Loss: 0.098 - Val Loss 0.310\nIter: 9000 - Train Loss: 1.281 - Val Loss 0.352\n\n\nOur prediction function looks similar to our training step, except that we use a final argmax to retrieve the class prediction.\n\n@torch.no_grad()\ndef pred_step(graph_sample, gnn, message_passing_iters=1):\n  gnn.eval()\n  nodes, edges = graph_sample.x, graph_sample.edge_attr\n  edge_list = graph_sample.edge_index.T\n  pred_per_node = gnn(nodes, edges, edge_list, message_passing_iters)\n  graph_pred_logits = pred_per_node.sum(dim=0, keepdim=True)\n  return graph_pred_logits.argmax()\n\n\n\nWe can print out the classification performance of this network.\nfrom functools import partial\nfrom sklearn.metrics import classification_report\n\ndef compute_classification_report(pred_fn, test_set, print_out=True):\n  preds = []\n  labels = []\n\n  for sample in test_set:\n    gt = sample.y\n    p = pred_fn(sample)\n    preds.append(p)\n    labels.append(gt)\n    \n  preds = torch.stack(preds)\n  labels = torch.stack(labels)\n  if print_out:\n    print(classification_report(labels, preds))\n  \n  return classification_report(labels, preds, output_dict=True)\n\n# Create a prediction function using our trained gnn\npred_fn = partial(pred_step, gnn=sgnn)\ncompute_classification_report(pred_fn, test_set);\n\n\n              precision    recall  f1-score   support\n\n           0       0.77      0.91      0.83        11\n           1       0.96      0.88      0.92        26\n\n    accuracy                           0.89        37\n   macro avg       0.86      0.90      0.88        37\nweighted avg       0.90      0.89      0.89        37\n\n\n\nNot bad! Our naive implementation already gets us to reasonable accuracies with relatively little tuning. One would play around with the hyperparamters such as number of layers, size of hidden dimensions, learning rate, batch size etc.\n\n\n\n\n\n\nNote\n\n\n\nBatch size..? The astute reader would have noticed that we have been training with a batch_size=1! This is because incorporating multiple batches requires us to carefully mask the input/output. Masking prevents information from leaking across samples and ensures for example that the output of index 13 only used the information of graph #13 and no one else. Since masking can be tricky, I have avoided it for now and left it as a post for the future. Or a task for the reader :)"
  },
  {
    "objectID": "posts/simplegnn-deux/index.html#do-multiple-messages-passes-help",
    "href": "posts/simplegnn-deux/index.html#do-multiple-messages-passes-help",
    "title": "Building Graph Neural Networks from Scratch - Part II",
    "section": "4 Do multiple messages passes help?",
    "text": "4 Do multiple messages passes help?\nLastly, I wanted to experiment with the effectivenes of multiple message passing iterations. Given our current setup, we can compare the performance easily.\n\n\nWe will train a new model for different values of message_passing_iters and gather the results. To save on time, I only trained each model for 5k iterations at lr=1e-3. All other hyperparamters are kept constant.\ndef single_train_run(message_passing_iters=1):\n    sgnn = SimpleGNN(input_node_features, input_edge_features,\n                    output_node_features=2, hidden_dims=32)\n    opt = torch.optim.AdamW(sgnn.parameters(), lr=1e-3, weight_decay=1e-5)\n\n    step_fn = partial(classification_step_fn, opt=opt,\n                      message_passing_iters=message_passing_iters)\n    for i in range(5_000):\n        idx = i % len(train_set)\n        graph_sample = train_set[idx]\n        loss = step_fn(graph_sample, sgnn)\n\n    pred_fn = partial(pred_step, gnn=sgnn, message_passing_iters=message_passing_iters)\n    return compute_classification_report(pred_fn, test_set, print_out=False)\n\nperf = []\nfor i in range(1, 11, 1):\n    metrics = single_train_run(message_passing_iters=i)\n    perf.append(metrics['weighted avg']['f1-score'])\n\nplt.bar(range(1, 11, 1), perf)\nplt.ylim(0.5, 1.0)\nplt.ylabel(\"F1-Score\")\nplt.xlabel(\"Number of Iterations\")\nsns.despine()\n\n\n\n\n\n\n\n\n\nIn the barplot above, I am plotting the F1 score, which measures the average predicttion performance of a model. It does seem that multiple rounds of message passing can help! However, its not clear which number would work well in practice. It probably depends on the dataset itself and whether incorporating global structure is helpful for the task. This experiment was a quick and dirty evaluation of performance. We would need to systematically perform multiple rounds of training and verify that number of iterations is indeed not improving performance. Nevertheless, we were able to squeeze a lot of juice from our humble mostly-linear-sometimes-graph-neural-network!"
  },
  {
    "objectID": "posts/simplegnn/index.html",
    "href": "posts/simplegnn/index.html",
    "title": "Building Graph Neural Networks from Scratch",
    "section": "",
    "text": "This is my attempt to understand Graph Neural Nets (GNNs), implementing the building blocks as I go. I have heavily relied on the seminal Neural Message Passing paper (Gilmer et al. 2017) and the excellent Distill post on GNNs (Sanchez-Lengeling et al. 2021) as my sources and I urge readers to look at them."
  },
  {
    "objectID": "posts/simplegnn/index.html#what-is-a-graph-anyway",
    "href": "posts/simplegnn/index.html#what-is-a-graph-anyway",
    "title": "Building Graph Neural Networks from Scratch",
    "section": "1 What is a graph anyway?",
    "text": "1 What is a graph anyway?\nGraphs will always show up in the form of nodes and edges. Either can have features. You can represent your data using a few building blocks:\n\nNode feature matrix\nEdge feature matrix\nGlobal feature vector (optional)\nGraph connectivity\n\nOption 1: Adjacency list\n\nAn array of node pairs \\((u, v)\\) representing an edge \\(u \\rightarrow v\\)\n\nOption 2: Adjacency matrix\n\nAn NxN matrix where each entry gets a 0/1 if there is an edge present in the graph\nNote that these matrices can be huge for large graphs so one may be forced to use adjacency lists in order to even fit the data into a network\n\n\n\nWe will see that connectivity information explicitly determines the types of operations and the order of operations to perform on the node / edge feature tensors.\n\n\n\n\n\n\nNote\n\n\n\nNote that you can have multiple ‘samples’ i.e. many graphs or a single graph. To get multiple samples/batches from a single large graph, you can sample a subgraph. There are many techniques to sample, which are not covered for now.\n\n\n\n1.1 A graph in the wild\nWe can download a public dataset from the torch_geometric package and view how the data is organized. This is the MUTAG which consists of graphs representing different molecules. We have labels associated with each molecule wwhich we can try to learn.\n\n\nCode to download the graph dataset\n# %pip install torch_geometric -q\n\nimport torch\nimport torch_geometric\nimport torch.nn as nn\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nDATASET_PATH = \"/tmp/data\"\n\n# Random number generator for later use\nrand_generator = torch.Generator()\nrand_generator.manual_seed(42)\ntorch.set_printoptions(precision=3)\n\ndataset = torch_geometric.datasets.TUDataset(\nroot=DATASET_PATH, name=\"MUTAG\",\nuse_edge_attr=True, use_node_attr=True\n)\nprint()\nprint(f'Dataset: {dataset}:')\nprint('====================')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')\n\n\nDownloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\nProcessing...\n\n\n\nDataset: MUTAG(188):\n====================\nNumber of graphs: 188\nNumber of features: 7\nNumber of classes: 2\n\n\nDone!\n\n\n\n\nWe can grab a single graph sample and display its properties\ndata = dataset[0] # Get the first graph object.\n\nprint()\nprint(data)\nprint('=============================================================')\n\n# Gather some statistics about the first graph.\nprint(f'Number of nodes: {data.num_nodes}')\nprint(f'Number of edges: {data.num_edges}')\nprint(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\nprint(f'Has isolated nodes: {data.has_isolated_nodes()}')\nprint(f'Has self-loops: {data.has_self_loops()}')\nprint(f'Is undirected: {data.is_undirected()}')\n\n\n\nData(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n=============================================================\nNumber of nodes: 17\nNumber of edges: 38\nAverage node degree: 2.24\nHas isolated nodes: False\nHas self-loops: False\nIs undirected: True\n\n\n\n\n1.2 Brief Aside: Why use graphs?\nThis is primarily a modeling decision. We want to emphasize the relational properties of the data, and edges essentially define a relation between two entities. We thus force our models to use this connectivity information in some manner. You could ‘flatten’ a graph into nodes with features, and convert each edge into a feature of the node. But this quickly becomes untenable and sparse ($n^2$ features for the existence of each edge alone + all sorts of edge features we are interested in).\n\n\n\nMolecules are often represented as graphs and for good reason. The relationships between each atom is precisely what gives a molecule its properties. The molecule shown above is called Vanillin. You might be able to guess its smell and taste!\n\n\nMolecules provide an illustrative example of the efficiency for modeling the data as graphs and not as “feature matrices”. How should we represent Vanillin in the image above? We could take one element at a time and keep a list of all its features and bonds: Carbon, [is_gas, is_connected_to_Oxygen, is_connected_to_Oxygen_with_double_bond, is_connected_to_Hydrogen, ...]. This could be a huge vector if all connections are considered.\nWe can even forget about the bonds, and the graph structuire will still emerge as an efficient scheme to represent the data. Say our molecule is composed of the elements Carbon, Oxygen, Iron and Aluminum. Should Iron and Aluminum have a ‘static’ feature for representing is_metal? If so, thenCarbon and Oxygen will also need to have a is_metal feature as part of the data representation. It might be more efficient to model “metallicness” as a relation between only the two entities that we care about. You could go on: should every entity have a feature saying is_nobel_gas or an edge connecting the few nobel gases in the dataset? Both techniques are valid, but we humans choose to feed our machines what we have deemed important."
  },
  {
    "objectID": "posts/simplegnn/index.html#graph-neural-networks",
    "href": "posts/simplegnn/index.html#graph-neural-networks",
    "title": "Building Graph Neural Networks from Scratch",
    "section": "2 Graph Neural Networks",
    "text": "2 Graph Neural Networks\nThe basic building blocks of a graph neural network include:\n\nEmbedding layers to learn rich representations of node / edge features\nMessage Passing layers to combine information using the connectivity defined by the graph\n\nThis is the meat and potatoes so to speak\n\nGraph “readout” layers from getting the final prediction\n\n\n2.1 Learn better representations of your input via Embedding Layers\nTransforming input features into inscrutable ‘rich’ representations is the bread and butter of deep learning. So we do the same. We can construct an embedding layer ourselves, using a weight matrix. You can think of this as an MLP / Feed Forward / torch. nn.Linear network with a single hidden layer and no bias. You could also just call it a linear transform but that doesn’t sound as fancy.\n\nclass Embedder:\n\n  def __init__(self, n_input, n_hidden):\n    weight_matrix = torch.randn(n_input, n_hidden,\n                       generator=rand_generator)\n    # Adjust weights to keep values small\n    weight_matrix /= n_hidden\n    self.W = torch.nn.Parameter(weight_matrix)\n\n  def __call__(self, x):\n    return torch.mm(x, self.W)\n\nhidden_dims = 5\nnode_embedder = Embedder(n_input=7, n_hidden=hidden_dims)\nedge_embedder = Embedder(n_input=4, n_hidden=hidden_dims)\n\n\n# Grab the nddes and edges from our graph sample\nnodes = data.x\nedges = data.edge_attr\nedge_list = data.edge_index.T\n\nnode_embs = node_embedder(nodes)\nedge_embs = edge_embedder(edges)\n\nnum_edges, num_edge_features = edge_embs.shape\nnode_embs.shape, edge_embs.shape\n\n(torch.Size([17, 5]), torch.Size([38, 5]))\n\n\n\n\n2.2 Combine features via Message Passing\nMessage passing layers are more conceptual than concrete layers like Attention or Convolution Kernels. In the context of GNNs, this concept was popularized by (Gilmer et al. 2017) in the Message Passing Neural Networks (MPNN) paper. The idea is to use the graph connections in some manner to aggregate information from neighbours.\nThe main idea is to take an edge \\(e: v \\rightarrow w\\) and pass the message from the target node \\(w\\) to the source node \\(v\\). We can also use information from the edge itself.\nUnder this lens, we can make the simplest form of message passing by adding information from neighbors and calling it a day.\n\nclass SimpleMessagePasser:\n\n  def __init__(self):\n    pass\n\n  def __call__(self, v, w, e_vw):\n    '''\n    Pass information TO v FROM w, additionally using edge information\n    This implementation does not use v\n    '''\n    m = w + e_vw\n    return m\n  \nmessenger = SimpleMessagePasser()\n\n\n2.2.1 Pass a single message along an edge\nTo see a message creation in action, we can grab an edge from our graph ancd compute embeddings for all entities involved.\n\n# Pick lucky number 13\nedge_idx = 13\n\n# The edge list stores index of the source and target nodes\nnode_v_idx, node_w_idx = edge_list[edge_idx]\n\n# We index into the embedding matrix and grab the corresponding features\nv, w = node_embs[node_v_idx], node_embs[node_w_idx]\ne = edge_embs[edge_idx]\n\n# These should all be of size `hidden_dimension` h\nv.shape, w.shape, e.shape\n\n(torch.Size([5]), torch.Size([5]), torch.Size([5]))\n\n\nPass messages from w to v using e\n\nmessenger(v, w, e).shape\n\ntorch.Size([5])\n\n\n\n\n2.2.2 Pass multiple messages across the entire graph\nOf course a graph has many edges and thus many messages to pass! Using a Tensor library like torch enables us to use clever indexing mechanisms to grab all the messages in one invocation.\n\n# Get indices for *all* nodes with edges\nnode_v_idxs, node_w_idxs = edge_list[:, 0], edge_list[:, 1]\nv, w = node_embs[node_v_idxs], node_embs[node_w_idxs]\n\n# Every edge in order\ne = edge_embs\n\n# Every message from every edge\nmessages = messenger(v, w, e)\nmessages.shape\n\ntorch.Size([38, 5])\n\n\nIf we take a look at all the source nodes, we see that they show up multiple times. This is because they have multiple edges.\n\nnode_v_idxs\n\ntensor([ 0,  0,  1,  1,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,  6,  6,  7,  7,\n         8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14,\n        15, 16])\n\n\nYou can do a quick and dirty count of neighbours for each node by constructing an adjacency matrix like below.\n\nA = torch.zeros(data.num_nodes, data.num_nodes)\nn1, n2 = data.edge_index\nA[n1, n2] = 1\nneighbor_counts = A.sum(axis=1, keepdim=True)\nneighbor_counts.flatten()\n\ntensor([2., 2., 2., 3., 3., 2., 2., 2., 3., 3., 2., 2., 3., 2., 3., 1., 1.])\n\n\nWe thus need to combine the messages or aggregate them in some manner. In deep learning lingo, you can also think of this as ‘pooling’ information from the neighboring nodes. We can also print out embeddings for the last three nodes just to see.\n\nprint(\"Node embeddings before message passing:\")\nprint(node_embs[-3:])\n\n# Aggregate / Pool information from neighboring nodes\n# respecting the neighborhood defined by the graph\n\nfor edge, h_message in zip(edge_list, messages):\n  # Note that node_w will be updated later\n  # when it is the source node in the edge\n  node_v_idx, node_w_idx = edge\n\n  # Aggregation method = sum\n  node_embs[node_v_idx] += h_message\n\n# Normalize by number of neighbors (optional but common)\nnode_embs /= neighbor_counts\n\n# Updated embeddings\nprint(\"========================\")\nprint(\"Node embeddings after message passing:\")\nprint(node_embs[-3:])\n\nNode embeddings before message passing:\ntensor([[-0.247, -0.009, -0.321, -0.150,  0.330],\n        [-0.078, -0.281, -0.146, -0.112, -0.154],\n        [-0.078, -0.281, -0.146, -0.112, -0.154]], grad_fn=&lt;SliceBackward0&gt;)\n========================\nNode embeddings after message passing:\ntensor([[-0.063, -0.038, -0.193, -0.096,  0.103],\n        [-0.554, -0.222, -0.617,  0.300,  0.248],\n        [-0.296, -0.243, -0.465, -0.291,  0.215]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that GNNs often perform many such message passing rounds. For this post, we will only focus on one but there is much flexibility in how one can perform multiple rounds. Again, message passign is more of a concept than a strict methodology.\n\n\n\n\n2.2.3 Brief Aside: Loops can be slow - use fancier methods instead\nThe above method to aggregate messages via iterating over the adjacency list will work but in practice, more efficient methods are used. In particular, Tensor operation libraries like PyTorch have built in methods such as scatter_reduce which will perform the update operations for you, while keeping track of gradients. From the torch documentation, we have the example below. The function allows us to update input (in-place) at position index by using the values from src.\n\nsrc = torch.tensor(  [1., 2., 3., 4., 5., 6.]).requires_grad_(True)\nindex = torch.tensor([0, 1, 0, 1, 2, 1])\ninput = torch.tensor([1., 2., 3., 4.])\ninput.scatter_reduce_(0, index, src, reduce=\"sum\")\n\ntensor([ 5., 14.,  8.,  4.], grad_fn=&lt;ScatterReduceBackward0&gt;)\n\n\nSo at position 0 of input, we have the sum of 1 (from input itself) + 1 (from src[0]) + 3 (from `src[2]). We can even compute gradients like below.\n\ninput[0].backward()\nsrc.grad # Which elements were used to update input 0?\n\ntensor([1., 0., 1., 0., 0., 0.])\n\n\nTo make the computation even clearer, we can step through the process of grabbing the appropriate indices from the edge list and applying the updates via scatter_reduce_.\n\n# We are building the index tensor that will define which node to update\nmessage_target_nodes = torch.zeros(num_edges, num_edge_features, dtype=torch.long)\n\n# Get target node indices from edge list\nmessage_target_nodes[:] = edge_list[:,0].reshape(-1,1)\n\n# E.g. row 3 can be read as: update node at index=1\n# using a message at index=2 from the message tensor\n# The column dimensions can be read as saying\n# ALL columns in message_row=3 will be used to update columns in node_row=1\nmessage_target_nodes[:4]\n\ntensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that we would ideally only need a message-index tensor of shape (num_edges, 1). However, the torch backprop only works when index exactly matches the message shape.\n\n\n\nnode_embs = node_embedder(nodes)\nnode_embs.scatter_reduce_(dim=0, index=message_target_nodes, src=messages,\n                           reduce='sum',include_self=True)\nnode_embs = node_embs / neighbor_counts\nnode_embs[-3:]\n\ntensor([[-0.063, -0.038, -0.193, -0.096,  0.103],\n        [-0.554, -0.222, -0.617,  0.300,  0.248],\n        [-0.296, -0.243, -0.465, -0.291,  0.215]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\n\n\n2.3 “Read out” the learned graph embeddings\nThe last block of our humble GNN will be a prediction layer. This will be another transformation to spit out the desired output of the model. In the case of classification, this layer will output the predictions for each class. This is often done per node and then aggregated to compute a graph-level prediction.\n\n# Graph-wise output to predict two classes\npredictor = Embedder(hidden_dims, 2)\ngraph_feature = predictor(node_embs)\noutput_logits = torch.sum(graph_feature, axis=0, keepdim=True)\noutput_logits\n\ntensor([[ -3.639, -18.362]], grad_fn=&lt;SumBackward1&gt;)\n\n\nCheck to see if we can backprop through the whole mechanism\n\noutput_logits.sum().backward()\n\nWe can see that the gradients exist for our edge embedding layer.\n\nedge_embedder.W.grad\n\ntensor([[ 0.731,  9.453, -0.921, 36.528,  7.506],\n        [ 0.107,  1.383, -0.135,  5.346,  1.098],\n        [ 0.071,  0.922, -0.090,  3.564,  0.732],\n        [ 0.000,  0.000,  0.000,  0.000,  0.000]])\n\n\nNote that we could have picked any flavor of prediction function. It could even output the same number of features as the original graph to create a Graph Autoencoder of sorts. this is precisely what we are going to do next to bring everything together!"
  },
  {
    "objectID": "posts/simplegnn/index.html#learn-to-reconstruct-an-input",
    "href": "posts/simplegnn/index.html#learn-to-reconstruct-an-input",
    "title": "Building Graph Neural Networks from Scratch",
    "section": "3 Learn to reconstruct an input",
    "text": "3 Learn to reconstruct an input\nOne of the best methods to test your deep learning model is to check and see whether it can “overfit” to a small batch or even a single sample. We have all the elements for our GNN. Now we can train it to learn to reconstruct the sample graph we have been playing around with.\n\n\nReinitialize our GNN layers and define functions to perform gradient descent\nrand_generator.manual_seed(42)\nhidden_dims = 3\ninput_node_features = 7\ninput_edge_features = 4\nlearning_rate = 1e-2\n\nnode_embedder = Embedder(n_input=input_node_features, n_hidden=hidden_dims)\nedge_embedder = Embedder(n_input=input_edge_features, n_hidden=hidden_dims)\npredictor = Embedder(hidden_dims, input_node_features)\n\nlayers = [\n      node_embedder,\n      edge_embedder,\n      predictor\n]\n\ndef zero_grad(layers):\n  for l in layers:\n    l.W.grad = None\n\n@torch.no_grad\ndef update(layers):\n  '''\n  A basic SGD step!\n  '''\n  for l in layers:\n    l.W.add_(l.W.grad, alpha=-learning_rate)\n\n\n\n# Get the entities of the graph\nnodes = data.x\nedges = data.edge_attr\nedge_list = data.edge_index.T\n\nfor i in range(10_000):\n  zero_grad(layers)\n\n  # Get embeddings\n  node_embs = node_embedder(nodes)\n  edge_embs = edge_embedder(edges)\n\n  # Get edge list and compute messages\n  node_v_idxs, node_w_idxs = edge_list[:,0], edge_list[:,1]\n  v_embs, w_embs = node_embs[node_v_idxs], node_embs[node_w_idxs]\n  messages = messenger(v_embs, w_embs, edge_embs)\n\n  # Get message-target node indices from edge list\n  # i.e. the nodes that will receive the message\n  num_edges, num_message_features = messages.shape\n  message_target_nodes = torch.zeros(num_edges, num_message_features,\n                                     dtype=torch.long, requires_grad=False)\n  message_target_nodes[:] = node_v_idxs.reshape(-1,1)\n\n  # Apply message updates from w to v\n  node_embs.scatter_reduce_(\n      dim=0, index=message_target_nodes,\n      src=messages, reduce='sum', include_self=True\n  )\n\n  # Normalize by number of neighbors\n  node_embs = node_embs / neighbor_counts\n\n  # Get reconstruction\n  node_reconstruction = predictor(node_embs)\n\n  # Compute reconstruction loss\n  loss = (nodes - node_reconstruction).square().sum(axis=1)\n  loss = loss.mean()\n  loss.backward()\n\n  # Gradient step over all parameters\n  update(layers)\n\n  if i % 1000 == 0:\n    print(f\"Step {i:4d}: {loss.item():.3f}\")\n\nStep    0: 1.025\nStep 1000: 0.079\nStep 2000: 0.077\nStep 3000: 0.074\nStep 4000: 0.070\nStep 5000: 0.061\nStep 6000: 0.044\nStep 7000: 0.028\nStep 8000: 0.019\nStep 9000: 0.015\n\n\n\n# Get the last reconstruction\nnode_reconstruction = predictor(node_embs).detach()\nnode_reconstruction.shape\n\ntorch.Size([17, 7])\n\n\n\nfig, axs = plt.subplots(1,2, figsize=(9,3))\nsns.heatmap(nodes, ax=axs[0]);\naxs[0].set_title(\"Original\")\nsns.heatmap(node_reconstruction, ax=axs[1]);\naxs[1].set_title(\"Reconstruction\");\n\n\n\n\nComparing our reconstruction to the original sample\n\n\n\n\nAs we can see the model is able to reproduice the input raesonably well! This is without any fancy optimization, using only one round of message passing and only addition as the message operator and no non-linearities. We actually have a linear model on our hands and it was able to reconstruct the input using the connectivity information from the graph. This is indeed a trite/trivial/contrived example but it gets the point across. We wanted to build a model that could use the structural information present in a graph and we were able to do so.\nWe can use our learnings here to start building more useful GNNs in the next part!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "thoughts",
    "section": "",
    "text": "Building Graph Neural Networks from Scratch - Part II\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\nAhsan Mahmood\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Graph Neural Networks from Scratch\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nAhsan Mahmood\n\n\n\n\n\n\nNo matching items"
  }
]